\documentclass{article}
\usepackage{amsmath}
\usepackage{indentfirst}
\usepackage{mathtools}
\setlength{\parindent}{2em}
\begin{document}
\title{Supervised Learning and Discriminative Algorithms}\author{jufan}\date{\today}
\maketitle
\tableofcontents

\section{Linear Regression}
% Regular paragraphs in a LaTex file are separated by (1) a blank line or (2) by the \par command
First we consider a simple situation where we want to approximate \emph{\textbf{y}} as a linear function of \emph{\textbf{x}}:
\begin{equation*}
h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2
\end{equation*}
Simplified format:
\begin{equation*}
h(x)=\sum\limits_{i=0}^n \theta_ix_i=\theta^Tx
\end{equation*}
where the $\theta_i$'s are the \emph{\textbf{parameters/weights}} with an extra \emph{\textbf{intercept term}} $\theta_0$ for $x_0=1$. ($\theta$ and $x$ are both $n+1$-dimentional vector)

Define the \emph{\textbf{cost function}} for \emph{\textbf{ordinary least squares}} regression model:
\begin{equation*}
J(\theta)=\frac{1}{2}\sum\limits_{i=1}^m(h_\theta(x^{(i)})-y^{(i)})^2
\end{equation*}
Our target: To choose $\theta$ so as to \emph{\textbf{minimize}} $J(\theta)$.
We use \emph{\textbf{gradient descent}} algorithm to solve this:
\begin{equation*}
\theta_j\coloneqq\theta_j-\alpha\frac{\partial}{\partial{\theta_j}}J(\theta)
\end{equation*}
Note that the update is \emph{\textbf{simultaneously}} performed for \emph{\textbf{all values}} of $j=0,\ldots,n$ , and $\alpha$ is the \emph{\textbf{learning rate}}.
\newline For $J(\theta)$ defined above, we have:
\begin{align*}
\frac{\partial}{\partial{\theta_j}}J(\theta) &= \frac{\partial}{\partial{\theta_j}}\frac{1}{2}(h_\theta(x)-y)^2 \\
                                             &= 2\cdot\frac{1}{2}(h_\theta(x)-y)\cdot\frac{\partial}{\partial{\theta_j}}(h_\theta(x)-y) \\
                                             &=
                                             (h_\theta(x)-y)\cdot\frac{\partial}{\partial{\theta_j}}\left(\sum\limits_{i=0}^n\theta_ix_i-y\right) \\
                                             &= (h_\theta(x)-y)x_j
\end{align*}

For a \textbf{single training example}, this gives the \emph{\textbf{LMS/Widrow-Hoff update rule}}:
\begin{equation*}
\theta_j\coloneqq\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
\end{equation*}

For a training set of \textbf{more than one example}, we have 2 types of gradient descent choices:
\begin{enumerate}
    \item \textbf{batch gradient descent}
        \newline Repeat until convergence \{
        \newline $\theta_j\coloneqq\theta_j+\alpha\sum\limits_{i=1}^m(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$ (for every j).
        \newline \}
    \item \textbf{stochastic gradient descent}
        \newline Loop \{
        \newline \hspace*{1cm} for $i=1$ to $m$, \{
        \newline \hspace*{1cm} \hspace{1cm}$\theta_j\coloneqq\theta_j+\alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}$ (for every j).
        \newline \hspace*{1cm}\}
        \newline \}
\end{enumerate}

In the original linear regression algorithm, to make prediction at a query point $x$ (to evaluate $h(x)$), we would:
\begin{enumerate}
    \item Fit $\theta$ to minimize $\sum\limits_i(y^{(i)}-\theta^Tx^{(i)})^2$.
    \item Output $\theta^Tx$.
\end{enumerate}
In contrast, the locally weighted linear regression algorithm would:
\begin{enumerate}
    \item Fit $\theta$ to minimize $\sum\limits_iw^{(i)}(y^{(i)}-\theta^Tx^{(i)})^2$.
    \item Output $\theta^Tx$.
\end{enumerate}

\section{Classification and logistic regression}
In this section, we focus on the \textbf{Binary classification} problem in which \emph{\textbf{y}} can only take two values, 0 and 1.

The first approach is to use the \emph{\textbf{logistic function/sigmoid function}}, and its form should be:
\begin{equation*}
h_\theta(x)=g(\theta^Tx)=\frac{1}{1+e^{-\theta^Tx}}
\end{equation*}
where
\begin{equation*}
g(z)=\frac{1}{1+e^{-z}}
\end{equation*}
One important property of sigmoid function:
\begin{align*}
g'(z) &= \frac{d}{dz}\frac{1}{1+e^{-z}} \\
      &= \frac{1}{{(1+e^{-z})}^2}(e^{-z}) \\
      &= \frac{1}{(1+e^{-z})}\left(1-\frac{1}{(1+e^{-z})}\right) \\
      &= g(z)(1-g(z))
\end{align*}

We fit the parameters for logistic regression model via maximum likelihood.
We assume that:
\begin{align*}
P(y=1\mid x;\theta) &= h_\theta(x) \\
P(y=0\mid x;\theta) &= 1 - h_\theta(x)
\end{align*}
which could be more compactly written as:
\begin{equation*}
p(y\mid x;\theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}
\end{equation*}
Thus under i.i.d assumption of the \emph{m} training examples, we have the likelihood of the parameters as:
\begin{align*}
L(\theta) &= p(\vec{y}\mid X;\theta) \\
          &= \prod\limits_{i=1}^mp(y^{(i)}\mid x^{(i)};\theta) \\
          &= \prod\limits_{i=1}^m\left(h_\theta(x^{(i)})\right)^{y^{(i)}}\left(1-h_\theta(x^{(i)})\right)^{1-y^{(i)}}
\end{align*}
It would be easier to maximize the log likelihood:
\begin{align*}
l(\theta) &= \log L(\theta) \\
          &= \sum\limits_{i=1}^my^{(i)}\log h(x^{(i)}) + (1-y^{(i)}) \log(1-h(x^{(i)}))
\end{align*}

Now we have the \emph{\textbf{stochastic gradient ascent rule}} (because we want the maximum of log likelihood):
\begin{equation*}
\theta_j\coloneqq\theta_j + \alpha(y^{(i)}-h_\theta(x^{(i)}))x_j^{(i)}
\end{equation*}

If we change logistic regression to force it output either 0 or 1, which means \emph{\textbf{g}} becomes:
\begin{equation*}
g(z)=\begin{cases}
     1 \quad if \ z\geq 0 \\
     0 \quad if \ z<0
     \end{cases}
\end{equation*}
With this modified definition of \emph{\textbf{g}} and the same update rule as above, we have the \emph{\textbf{Perceptron learning algorithm}}.

\section{Generalized Linear Models}
Both regression ($y\mid x;\theta\sim N(\mu,\sigma^2)$) and classification methods ($y\mid x;\theta\sim Bernouli(\phi)$) are special cases of \emph{\textbf{Generalized Linear Models}}.

A class of distributions is in the \emph{\textbf{Exponential family}} if it can be written as:
\begin{equation*}
p(y;\eta)=b(y)\exp(\eta^TT(y)-a(\eta))
\end{equation*}
Here \emph{\textbf{$\eta$}} is called \emph{\textbf{natural parameter/canonical parameter}} of the distribution; $T(y)$ is called \emph{\textbf{sufficient statistic}}; $a(\eta)$ is called \emph{\textbf{log partition function}}, and $e^{-a(\eta)}$ is a \emph{\textbf{normalization constant}} that ensures the distribution $p(y;\eta)$ sums/integrates over y to 1.

Now the Bernoulli distributions can be written as:
\begin{align*}
p(y;\phi) &= \phi^y(1-\phi)^{1-y} \\
          &= \exp(y\log\phi+(1-y)\log(1-\phi)) \\
          &= \exp\left(\left(\log\left(\frac{\phi}{1-\phi}\right)\right)y+\log(1-\phi)\right)
\end{align*}

And the Gaussian distributions can be written as:
\begin{align*}
p(y;\mu) &= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}(y-\mu)^2\right) \\
         &= \frac{1}{\sqrt{2\pi}}\exp\left(-\frac{1}{2}y^2\right)\cdot\exp\left(\mu y-\frac{1}{2}\mu^2\right)
\end{align*}
\end{document} 
