\documentclass{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{setspace}
\setlength{\parindent}{2em}
\begin{document}
\title{Generative Algorithms}\author{jufan}\date{\today}
\maketitle
\tableofcontents

\section{Generative Learning algorithms}
\emph{\textbf{Discriminative learning algorithms}} are algorithms that try to:
\begin{itemize}
  \item learn $p(y\mid x)$ directly (such as logistic regression)
  \item learn mappings directly from the space of inputs $\chi$ to the labels $\{0,1\}$ (such as perceptron algorithm)
\end{itemize}

\emph{\textbf{Generative learning algorithms}} are algorithms that try to model $p(x\mid y)$ and $p(y)$

\begin{spacing}{2}
\end{spacing}

\emph{\textbf{Bayes rule}} is used to derived the posterior distribution on $y$ given $x$, given the \emph{\textbf{class priors}} $p(y)$ and conditional probability $p(x\mid y)$:
\begin{equation*}
p(y\mid x) = \frac{p(x\mid y)p(y)}{p(x)}
\end{equation*}
The denominator $p(x)$ is calculated by summation of all possible numerators, and can be ignored when calculating $p(y\mid x)$ to make prediction.

The \emph{\textbf{multivariate normal distribution}} in $n$-dimensions, also called the \emph{\textbf{multivariate Gaussian distribution}}, is parameterized by a \emph{\textbf{mean vector}} $\mu\in\mathbb{R}^n$ and a \emph{\textbf{covariance matrix}} $\Sigma\in\mathbb{R}^{n\times m}$, where $\Sigma \geq 0$ is symmetric and positive semi-definite.

Probability density of multivariate normal distribution is given by:
\begin{equation*}
p(x;\mu,\Sigma)=\frac{1}{(2\pi)^{n/2}\lvert\Sigma\rvert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\right)
\end{equation*}
in which $\lvert\Sigma\rvert$ is the determinant of the matrix $\Sigma$, and "$N(\mu,\Sigma)$" can be used the same as "$p(x;\mu,\Sigma)$".

For a random variable $X$ distributed $N(\mu, \Sigma)$, the \emph{\textbf{mean}} is given by $\mu$:
\begin{equation*}
E[X]=\int_x xp(x;\mu,\Sigma)dx=\mu
\end{equation*}
For a random vector-valued variable $Z$, the \emph{\textbf{covariance}} is given by:
\begin{equation*}
Cov(Z)=E[(Z-E[Z])(Z-E[Z])^T]=E[ZZ^T]-(E[Z])(E[Z])^T
\end{equation*}
if $X\sim N(\mu,\Sigma)$, then $Cov(X)=\Sigma$.

A Gaussian with zero mean and identity covariance is called the \emph{\textbf{standard normal distribution}}.

\emph{\textbf{Gaussian Discriminant Analysis (GDA) model}} is used to deal with problems in which the input features $x$ are continuous-valued random variables.
The model is:
\begin{align*}
y\ &\sim\ Bernoulli(\phi) \\
x\mid y=0\ &\sim\ N(\mu_0,\Sigma) \\
x\mid y=1\ &\sim\ N(\mu_1,\Sigma) 
\end{align*}
And we have distributions as:
\begin{align*}
p(y)\ &=\ \phi^y(1-\phi)^{1-y} \\
p(x\mid y=0)\ &=\ \frac{1}{(2\pi)^{n/2}\lvert\Sigma\rvert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_0)^T\Sigma^{-1}(x-\mu_0)\right) \\
p(x\mid y=1)\ &=\ \frac{1}{(2\pi)^{n/2}\lvert\Sigma\rvert^{1/2}}\exp\left(-\frac{1}{2}(x-\mu_1)^T\Sigma^{-1}(x-\mu_1)\right)
\end{align*}
Parameters of \emph{\textbf{Gaussian Discriminant Analysis (GDA)}} model are: \\ 
$\phi, \Sigma, \mu_0, \mu_1$.
The log-likelihood of the data is given by:
\begin{align*}
l(\phi, \mu_0, \mu_1, \Sigma)\ &=\ \log\prod_{i=1}^mp(x^{(i)},y^{(i)};\phi,\mu_0,\mu_1,\Sigma) \\
                               &=\ \log\prod_{i=1}^mp(x^{(i)}\mid y^{(i)};\phi,\mu_0,\mu_1,\Sigma)p(y^{(i)};\phi)
\end{align*}
By maximizing $l$, we find the maximum likelihood estimate of the parameters above.

\begin{spacing}{2}
\end{spacing}

In GDA, feature vectors $x$ are continuous, real-valued vectors, and in Naive Bayes $x$ are discrete-valued.

\begin{spacing}{2}
\end{spacing}

To model $p(x\mid y)$, we make a very strong assumption (\emph{\textbf{Naive Bayes (NB) assumption}}): the $x_i$'s are conditionally independent given $y$. The resulting algorithm is called the \emph{\textbf{Naive Bayes classifier}}.

Parameters of \emph{\textbf{Naive Bayes (NB)}} model are $\phi_{i\mid y=1}=p(x_i=1\mid y=1), \phi_{i\mid y=0}=p(x_i=1\mid y=0), \phi_y=p(y=1)$, and given the training set $\{(x^{(i)},y^{(i)});i=1,\ldots,m\}$, we have the joint likelihood of the data (which should be maximized to get the maximum likelihood estimates of parameters):
\begin{equation*}
L(\phi_{y},\phi_{i\mid y=0},\phi_{i\mid y=1})=\prod_{i=1}^mp(x^{(i)},y^{(i)})
\end{equation*}
After fitting these parameters, to make a prediction on a new example with features $x$, we only need to calculate:
\begin{align*}
p(y=1\mid x)\ &=\ \frac{p(x\mid y=1)p(y=1)}{p(x)} \\
              &=\ \frac{\left(\prod\limits_{i=1}^np(x_i\mid y=1)\right)p(y=1)}{\left(\prod\limits_{i=1}^np(x_i\mid y=1)\right)p(y=1)+\left(\prod\limits_{i=1}^np(x_i\mid y=0)\right)p(y=0)}
\end{align*}
then pick whichever class that has the highest posterior probability.

\end{document} 
